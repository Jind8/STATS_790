@inproceedings{10.1145/2939672.2939785,
  title = {{{XGBoost}}: {{A}} Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  series = {{{KDD}} '16},
  pages = {785--794},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  isbn = {978-1-4503-4232-2},
  keywords = {large-scale machine learning}
}

@manual{auguieGridExtraMiscellaneousFunctions2017,
  type = {Manual},
  title = {{{gridExtra}}: {{Miscellaneous}} Functions for "{{Grid}}" Graphics},
  author = {Auguie, Baptiste},
  year = {2017}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash 156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.}
}

@misc{bujokasGradientBoostingPython2022,
  title = {Gradient {{Boosting}} in {{Python}} from {{Scratch}}},
  author = {Bujokas, Eligijus},
  year = {2022},
  journal = {Medium}
}

@misc{dataset,
  title = {Early Stage Diabetes Risk Prediction Dataset.},
  year = {2020},
  publisher = {{UCI Machine Learning Repository}}
}

@manual{kuhnCaretClassificationRegression2022a,
  type = {Manual},
  title = {Caret: {{Classification}} and Regression Training},
  author = {Kuhn, Max},
  year = {2022}
}

@manual{kuhnTidymodelsCollectionPackages2020,
  type = {Manual},
  title = {Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles.},
  author = {Kuhn, Max and Wickham, Hadley},
  year = {2020}
}

@article{liawClassificationRegressionRandomForest2002,
  title = {Classification and Regression by {{randomForest}}},
  author = {Liaw, Andy and Wiener, Matthew},
  year = {2002},
  journal = {R News},
  volume = {2},
  number = {3},
  pages = {18--22}
}

@article{meyerStrucplotFrameworkVisualizing2006,
  title = {The Strucplot Framework: {{Visualizing}} Multi-Way Contingency Tables with Vcd},
  author = {Meyer, David and Zeileis, Achim and Hornik, Kurt},
  year = {2006},
  journal = {Journal of Statistical Software},
  volume = {17},
  number = {3},
  pages = {1--48},
  doi = {10.18637/jss.v017.i03}
}

@manual{meyerVcdVisualizingCategorical2023,
  type = {Manual},
  title = {Vcd: {{Visualizing}} Categorical Data},
  author = {Meyer, David and Zeileis, Achim and Hornik, Kurt},
  year = {2023}
}

@inproceedings{paper,
  title = {Likelihood Prediction of Diabetes at Early Stage Using Data Mining Techniques},
  booktitle = {Computer Vision and Machine Intelligence in Medical Image Analysis},
  author = {Islam, M. M. Faniqul and Ferdousi, Rahatara and Rahman, Sadikur and Bushra, Humayra Yasmin},
  editor = {Gupta, Mousumi and Konar, Debanjan and Bhattacharyya, Siddhartha and Biswas, Sambhunath},
  year = {2020},
  pages = {113--125},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  abstract = {DiabetesIslam, M M Faniqul is one of the fastest growing chronic life threatening diseases that haveFerdousi, Rahatara already affected 422 million people worldwide according to the report of World HealthRahman, Sadikur Organization (WHO), in 2018. Due to the presence of a relatively longBushra, Humayra Yasmin asymptomatic phase, early detection of diabetes is always desired for a clinically meaningful outcome. Around 50\% of all people suffering from diabetes are undiagnosed because of its long-term asymptomatic phase. The early diagnosis of diabetes is only possible by proper assessment of both common and less common sign symptoms, which could be found in different phases from disease initiation up to diagnosis. Data mining classification techniques have been well accepted by researchers for risk prediction model of the disease. To predict the likelihood of having diabetes requires a dataset, which contains the data of newly diabetic or would be diabetic patient. In this work, we have used such a dataset of 520 instances, which has been collected using direct questionnaires from the patients of Sylhet Diabetes Hospital in Sylhet, Bangladesh. We have analyzed the dataset with Naive Bayes Algorithm, Logistic Regression Algorithm, and Random Forest Algorithm and after applying tenfold Cross- Validation and Percentage Split evaluation techniques, Random forest has been found having best accuracy on this dataset. Finally, a commonly accessible, user-friendly tool for the end user to check the risk of having diabetes from assessing the symptoms and useful tips to control over the risk factors has been proposed.},
  isbn = {978-981-13-8798-2}
}

@book{tibshiraniElementsStatisticalLearning2001,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}} : With 200 {{Full-color Illustrations}}},
  author = {Tibshirani, R. and Hastie, T. and Friedman, J.H.},
  year = {2001},
  series = {Springer Series in Statistics},
  publisher = {{Springer}},
  isbn = {978-1-4899-0519-2}
}

@manual{wickhamDplyrGrammarData2023,
  type = {Manual},
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  author = {Wickham, Hadley and Fran{\c c}ois, Romain and Henry, Lionel and M{\"u}ller, Kirill and Vaughan, Davis},
  year = {2023}
}

@book{wickhamGgplot2ElegantGraphics2016c,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@manual{wickhamReadrReadRectangular2023,
  type = {Manual},
  title = {Readr: {{Read}} Rectangular Text Data},
  author = {Wickham, Hadley and Hester, Jim and Bryan, Jennifer},
  year = {2023}
}

@article{zeileisResidualbasedShadingsVisualizing2007a,
  title = {Residual-Based Shadings for Visualizing (Conditional) Independence},
  author = {Zeileis, Achim and Meyer, David and Hornik, Kurt},
  year = {2007},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {16},
  number = {3},
  pages = {507--525},
  doi = {10.1198/106186007X237856}
}
